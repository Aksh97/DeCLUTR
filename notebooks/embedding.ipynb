{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding text with an existing model\n",
    "\n",
    "This notebook will walk you through embedding some text with a pretrained model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR). You can embed text in one of three ways:\n",
    "\n",
    "1. __As a library__: import and initialize an object from this repo, which can be used to embed sentences/paragraphs.\n",
    "2. __ü§ó Transformers__: load our pretrained model with the [ü§ó Transformers library](https://github.com/huggingface/transformers).\n",
    "3. __Bulk embed__: embed all text in a given text file with a simple command-line interface.\n",
    "\n",
    "Each approach has advantages and disadvantages:\n",
    "\n",
    "1. __As a library__: This is the easiest way to add DeCLUTR to an existing pipeline, but requires that you install our package.\n",
    "2. __ü§ó Transformers__: This only requires you to install the [ü§ó Transformers library](https://github.com/huggingface/transformers), but requires more boilerplate code.\n",
    "3. __Bulk embed__: This most suitable if you want to embed large quantities of text \"offline\" (e.g. not on-the-fly within an existing pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Install the prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/JohnGiorgi/DeCLUTR.git\n",
    "pip install DeCLUTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, please install AllenNLP from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/allenai/allennlp.git\n",
    "cd allennlp\n",
    "pip install -e .\n",
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check to see if we have a GPU available, which we can use to dramatically speed up the embedding of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda_device = torch.cuda.current_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    cuda_device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ As a library\n",
    "\n",
    "To use the model as a library, import `Encoder` and pass it some text (it accepts both strings and lists of strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from declutr import Encoder\n",
    "\n",
    "# This can be a path on disk to a model you have trained yourself OR\n",
    "# the name of one of our pretrained models.\n",
    "pretrained_model_or_path = \"declutr-small\"\n",
    "\n",
    "encoder = Encoder(pretrained_model_or_path, cuda_device=cuda_device)\n",
    "embeddings = encoder([\n",
    "    \"A smiling costumed woman is holding an umbrella.\",\n",
    "    \"A happy woman in a fairy costume holds an umbrella.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these embeddings can then be used, for example, to compute the semantic similarity between some number of sentences or paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "print(semantic_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the list of available `PRETRAINED_MODELS` in [declutr/encoder.py](https://github.com/JohnGiorgi/DeCLUTR/blob/master/declutr/encoder.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from declutr.encoder import PRETRAINED_MODELS ; print(list(PRETRAINED_MODELS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ ü§ó Transformers\n",
    "\n",
    "Our pretrained models are also hosted with ü§ó Transformers, so they can be used like any other model in that library. Here is a simple example using [DeCLUTR-small](https://huggingface.co/johngiorgi/declutr-small):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-small\")\n",
    "model = AutoModel.from_pretrained(\"johngiorgi/declutr-small\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare some text to embed\n",
    "text = [\n",
    "    \"A smiling costumed woman is holding an umbrella.\",\n",
    "    \"A happy woman in a fairy costume holds an umbrella.\",\n",
    "]\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Put the tensors on the GPU, if available\n",
    "for name, tensor in inputs.items():\n",
    "    inputs[name] = tensor.to(model.device)\n",
    "\n",
    "# Embed the text\n",
    "with torch.no_grad():\n",
    "    sequence_output, _ = model(**inputs, output_hidden_states=False)\n",
    "\n",
    "# Mean pool the token-level embeddings to get sentence-level embeddings\n",
    "embeddings = torch.sum(\n",
    "    sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n",
    ") / torch.clamp(torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)\n",
    "\n",
    "# Compute a semantic similarity via the cosine distance\n",
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "print(semantic_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently available models:\n",
    "\n",
    "- [johngiorgi/declutr-small](https://huggingface.co/johngiorgi/declutr-small)\n",
    "- johngiorgi/declutr-base (üîú)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Bulk embed a file\n",
    "\n",
    "First, lets save our running example to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"A smiling costumed woman is holding an umbrella.\\nA happy woman in a fairy costume holds an umbrella.\" > \"input.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need a pretrained model to embed the text with. Following our running example, lets use DeCLUTR-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.file_utils import cached_path\n",
    "from declutr.encoder import PRETRAINED_MODELS\n",
    "\n",
    "# Download the model OR retrieve its filepath if it has already been downloaded & cached.\n",
    "declutr_small_cached_path = cached_path(PRETRAINED_MODELS[\"declutr-small\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed all text in a given file with a trained model, run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When embedding text with a pretrained model, we do NOT want to sample spans.\n",
    "# We can turn off span sampling by setting the num_anchors attribute to None.\n",
    "overrides = \"{'dataset_reader.num_anchors': null}\"\n",
    "\n",
    "!allennlp predict $declutr_small_cached_path \"input.txt\" \\\n",
    "    --output-file \"embeddings.jsonl\" \\\n",
    "    --batch-size 32 \\\n",
    "    --cuda-device $cuda_device \\\n",
    "    --use-dataset-reader \\\n",
    "    --overrides \"$overrides\" \\\n",
    "    --include-package \"declutr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, lets load the embeddings and make sure their cosine similarity is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"embeddings.jsonl\", \"r\") as f:\n",
    "    embeddings = []\n",
    "    for line in f:\n",
    "        embeddings.append(json.loads(line)[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "print(semantic_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôªÔ∏è Conclusion\n",
    "\n",
    "That's it! In this notebook, we covered three ways to embed text with a pretrained model. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:declutr] *",
   "language": "python",
   "name": "conda-env-declutr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
