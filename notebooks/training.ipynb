{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8jt6ML03DS5"
   },
   "source": [
    "# Training your own model\n",
    "\n",
    "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SU3Iod2-g0-o"
   },
   "source": [
    "## üîß Install the prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sr4r5pN40Kli"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/JohnGiorgi/DeCLUTR.git\n",
    "!pip install DeCLUTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, please install AllenNLP from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/allenai/allennlp.git\\n\",\n",
    "%cd allennlp\n",
    "!pip install -e .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zog7ApwuUD7_"
   },
   "source": [
    "## üìñ Preparing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwnLpUmN4Art"
   },
   "source": [
    "\n",
    "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
    "\n",
    "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
    "\n",
    "```python\n",
    "min_length = num_anchors * max_span_len * 2\n",
    "```\n",
    "\n",
    "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0fwnwq23aAZ"
   },
   "outputs": [],
   "source": [
    "train_data_path = \"/content/wikitext_103/train.txt\"\n",
    "min_length = 2048\n",
    "\n",
    "!python DeCLUTR/scripts/preprocess_wikitext_103.py $train_data_path --min-length $min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jH3JAOBy_5dl"
   },
   "source": [
    "By default, [`allennlp train`](https://docs.allennlp.org/master/api/commands/train/) will create a vocabulary for our dataset. Because our model comes with a pretrained vocabulary, we can skip this step by creating the following file under our dataset folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gh46STde_OTz"
   },
   "outputs": [],
   "source": [
    "vocabulary_directory = \"/content/wikitext_103/vocabulary\"\n",
    "!mkdir -p $vocabulary_directory\n",
    "!echo -e \"*tags\\n*labels\" > \"$vocabulary_directory/non_padded_namespaces.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUEFeupP6qy-"
   },
   "source": [
    "Lets confirm that our dataset looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7ffGXCn7Cpq"
   },
   "outputs": [],
   "source": [
    "!wc -l $train_data_path  # This should be approximately 17.8K lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10DprWZc9iV6"
   },
   "outputs": [],
   "source": [
    "!head -n 1 $train_data_path  # This should be a single Wikipedia entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKYdambZ59nM"
   },
   "source": [
    "## üèÉ Training the model\n",
    "\n",
    "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. DeCLUTR provides a handful of these config files with sensible defaults. Let's look at a simplified config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTaSExh4ba8e"
   },
   "outputs": [],
   "source": [
    "with open(\"DeCLUTR/configs/contrastive_simple.jsonnet\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-f1HqWSscWOx"
   },
   "source": [
    "\n",
    "The only thing to configure is the `train_data_path`, and optionally, the `vocabulary`. Because our vocabulary is pretrained, specifying it here will prevent AllenNLP from trying to construct it again. Here, we will pass both arguments to `allennlp train` via the `--overrides` argument, but you can also provide it in your config file directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YS9VuxESBcr3"
   },
   "outputs": [],
   "source": [
    "overrides = (\n",
    "    f\"{{'train_data_path': '{train_data_path}', \"\n",
    "    f\"'vocabulary': {{'type': 'from_files', 'directory': '{vocabulary_directory}'}}}}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Db_cNfZ76KRf"
   },
   "outputs": [],
   "source": [
    "!allennlp train \"DeCLUTR/configs/contrastive_simple.jsonnet\" \\\n",
    "    --serialization-dir \"output\" \\\n",
    "    --overrides \"$overrides\" \\\n",
    "    --include-package \"declutr\" \\\n",
    "    -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eD5dZo18EE-S"
   },
   "source": [
    "## ‚ôªÔ∏è Conclusion\n",
    "\n",
    "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:declutr] *",
   "language": "python",
   "name": "conda-env-declutr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
